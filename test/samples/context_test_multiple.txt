context machine learning model optimization for edge devices um so I was reviewing the performance metrics from our latest tests and in the context of our current hardware limitations we really need to consider quantization I mean the models are just too big to run efficiently on these edge devices and the broader context here is that our competitors are already shipping products with optimized models so we're falling behind in terms of performance the context window for these devices is really limited too which adds another layer of complexity to the optimization process I think we need to look at techniques like knowledge distillation or maybe pruning to get the model size down while maintaining accuracy in the specific context of our use case which is real time inference on battery powered devices